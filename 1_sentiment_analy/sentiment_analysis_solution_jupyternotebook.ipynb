{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 28 19:06:18 2020\n",
    "\n",
    "@author: shahzeb\n",
    "\n",
    "IDE: Spyder(Python3.8)\n",
    "\"\"\"\n",
    "\"\"\"  \n",
    "Introduction: From the problem statement the required is as follows: F1-Score and Accuracy score\n",
    "for the given sentiment_analysis.txt. The file contains column0 which has corpus sentences and \n",
    "column1 which has the emotion for 0:Negative and 1:Positive.\n",
    "\n",
    "The given dataset is in the form of Labeled data, hence supervised learning is obvious for data. And\n",
    "pre-processing the data implementing Classification Algorithms is a good way to train\n",
    "and test models for this paticular dataset.\n",
    "\n",
    "Steps followed in this code assignment:\n",
    "   \n",
    "    A) Pre-Processing:\n",
    "        \n",
    "        1) Tokenizing: The dataset is tokenized with 3 different tokenizers. Namely, Word_tokenizer,\n",
    "            Treebank_tokenizer and Regexp_tokenizer. And after that pre-processing step-2,3,4 are applied to\n",
    "            all the 3 tokenizers. \n",
    "        2) Removing the Stop words.\n",
    "        3) POS Tagging: It's resonable to do POS tagging after removing stop words as it'll remove\n",
    "            unimportant words, thus we have to process only important words: saving time and computation power.\n",
    "        4) After that lemmatization(WordNetLemmatizer is used in this assignment) is chosen over \n",
    "            stemmitization as it'll give the root word for any given word in the'\n",
    "        \n",
    "        5)Data Cleaning: Numbers,Hyphens,quotation marks,new line characters,apostrophes,etc \n",
    "            are removed from the txt.\n",
    "    \n",
    "    Note: I wanted to experiment with different tokenizers, lemmatizer,etc. The 3 different tokenizers \n",
    "    were considered, but only 1 lemmatizer was used throughout this project as the result from the\n",
    "    tokenizers were considerably similar, varying by 4-5% , So given the time constraint I \n",
    "    only tried to do implement the Naive Bayers Classifier in CBOW for 3 tokenizers. And for \n",
    "    remining only Treebank Tokenizer was used along with WordNetlemmatizer.\n",
    "    \n",
    "    \n",
    "    B) Splitting Dataset/Corpus:\n",
    "        The dataset/corpus is split into 66.6% of Training data and 33.3% of test data. \n",
    "        Moreover, the data is shuffledeverytime the code is re-run such that different instances \n",
    "        are shuffled between test and train datasets.\n",
    "    C) Feature Engineering: In order to run machine learning algorithms we need to convert \n",
    "    the text files into numerical feature vectors. Bag of Words (CountVectorizer()) is used\n",
    "    in 1st for Naive Bayers and Random Forest Algorithms. A term-Document Matrix was constructed \n",
    "    from the CountVectorizer() containing words or terms along columns and sentences along rows.\n",
    "    The bag of words only considers the count of words which is not a good practice for doing sentiment analysis. \n",
    "    Because some common words appear in many sentences which contain less importantnce. \n",
    "    Therefore,TF-IDF is used later in this assignment for Naive Bayers and Random Forest Algorithms\n",
    "    which takes into account the word based upon its uniqueness.\n",
    "    \n",
    "    D) Model Construction: MultiNomial Naive Bayers was constructed from sklearn.model_selection\n",
    "    library and Random Forest Classifier was constructed from sklearn.ensemble.\n",
    "    E) Evaluation: The accuracy_score, F1-scores are printed into the console. As well as confusion matrix is also calculated\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "import re\n",
    "import nltk\n",
    "import warnings \n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize,TreebankWordTokenizer\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "import pandas as pd\n",
    "X = pd.read_csv('sentiment_analysis.txt', sep=\"\\t\", header=None)\n",
    "target=X[1]\n",
    "#taking the output into target variable\n",
    "#To print the corpus from the dataset\n",
    "#corpus=X[0]\n",
    "#for line in corpus:\n",
    "#    print(line)\n",
    "\n",
    "#importing stopwords library to later remove the stop words from our corpus\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer    #WordLemmatizer is used to cut down the word back to it's root word\n",
    "from nltk.corpus import wordnet            #wordnet is a lexical database for the English language, we'll use it to find POS tags for words\n",
    "lemmatizer = WordNetLemmatizer()          # creating instance of class WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus dataset, Column 0: is for Sentence Corpus and  Column 1: is for emotions. For 0 value in Column 1 it's negative sentiment for the sentence and for value 1 its positive sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1\n",
       "0    So there is no way for me to plug it in here i...  0\n",
       "1                          Good case, Excellent value.  1\n",
       "2                               Great for the jawbone.  1\n",
       "3    Tied to charger for conversations lasting more...  0\n",
       "4                                    The mic is great.  1\n",
       "..                                                 ... ..\n",
       "995  The screen does get smudged easily because it ...  0\n",
       "996  What a piece of junk.. I lose more calls on th...  0\n",
       "997                       Item Does Not Match Picture.  0\n",
       "998  The only thing that disappoint me is the infra...  0\n",
       "999  You can not answer calls with the unit, never ...  0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The corpus dataset, Column 0: is for Sentence Corpus and  Column 1: is for emotions. For 0 value in Column 1 it's negative sentiment for the sentence and for value 1 its positive sentiment\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                            # (A) PRE-PROCESSING\n",
    "#The function below converts nltk tag to wordnet tags for POS tagging.\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# The function below is used for tokenization followed by removing stop words, POS Tagging words, Lemmatization. This function is for Wordtokenizer only.\n",
    "def Wordtoken_lemmatize_sentence(sentence):\n",
    "   \n",
    "    tokenizer_word= word_tokenize(str(sentence))    #Tokenize the sentence \n",
    "    filtered_sentence = ' '.join([w for w in tokenizer_word if not w in stop_words]) #After tokenizing find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(filtered_sentence))  #nltk_tagged contains the POS tag that will be used by lemmatizer to effectively find root words\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #If no available POS tag then append the token as it is.\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #Else use the POS tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag).lower()) #appending the words to sentances\n",
    "    return \" \".join(lemmatized_sentence)  #returning the lematized sentance\n",
    "\n",
    "X['Wordtoken_cleaned_txt']=X[0].apply(Wordtoken_lemmatize_sentence)  #calling the above function(Wordtoken_lemmatize_sentence) and stores the values to X dataset in a new column named as Wordtoken_cleaned_txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Wordtoken_cleaned_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>so way plug us unless I go converter .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>good case , excellent value .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>great jawbone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>tied charger conversation last 45 minutes.majo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>The mic great .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The screen get smudge easily touch ear face .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>What piece junk .. I lose call phone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "      <td>item does not match picture .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "      <td>The thing disappoint infra red port ( irda ) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>You answer call unit , never work !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0    So there is no way for me to plug it in here i...  0   \n",
       "1                          Good case, Excellent value.  1   \n",
       "2                               Great for the jawbone.  1   \n",
       "3    Tied to charger for conversations lasting more...  0   \n",
       "4                                    The mic is great.  1   \n",
       "..                                                 ... ..   \n",
       "995  The screen does get smudged easily because it ...  0   \n",
       "996  What a piece of junk.. I lose more calls on th...  0   \n",
       "997                       Item Does Not Match Picture.  0   \n",
       "998  The only thing that disappoint me is the infra...  0   \n",
       "999  You can not answer calls with the unit, never ...  0   \n",
       "\n",
       "                                 Wordtoken_cleaned_txt  \n",
       "0               so way plug us unless I go converter .  \n",
       "1                        good case , excellent value .  \n",
       "2                                      great jawbone .  \n",
       "3    tied charger conversation last 45 minutes.majo...  \n",
       "4                                      The mic great .  \n",
       "..                                                 ...  \n",
       "995      The screen get smudge easily touch ear face .  \n",
       "996             What piece junk .. I lose call phone .  \n",
       "997                      item does not match picture .  \n",
       "998     The thing disappoint infra red port ( irda ) .  \n",
       "999                You answer call unit , never work !  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below is used for tokenization followed by removing stop words, POS Tagging words, Lemmatization. This function is for TreeBank Tokenizer only.\n",
    "def Treetoken_lemmatize_sentence(sentence):\n",
    "    treebank_tokenizer= TreebankWordTokenizer().tokenize(str(sentence))   #Tokenize the sentence \n",
    "    filtered_sentence = ' '.join([w for w in treebank_tokenizer if not w in stop_words]) #After tokenizing find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(filtered_sentence))    #nltk_tagged contains the POS tag that will be used by lemmatizer to effectively find root words\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "             #If no available POS tag then append the token as it is.\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #Else use the POS tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag).lower())  #appending the words to sentances\n",
    "    return \" \".join(lemmatized_sentence)                    #returning the lematized sentance\n",
    "\n",
    "X['TreeToken_cleaned_txt']=X[0].apply(Treetoken_lemmatize_sentence)  #calling the above function(Treetoken_lemmatize_sentence) and stores the values to X dataset in a new column named as TreeToken_cleaned_txt.\n",
    "\n",
    "Regtokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# The function below is used for tokenization followed by removing stop words, POS Tagging words, Lemmatization. This function is for RegExp Tokenizer only.\n",
    "def Regextoken_lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    regex_token=Regtokenizer.tokenize(str(sentence.lower()))    #Tokenize the sentence \n",
    "    filtered_sentence = ' '.join([w for w in regex_token if not w in stop_words])  #After tokenizing find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(filtered_sentence))     #nltk_tagged contains the POS tag that will be used by lemmatizer to effectively find root words\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "             #If no available POS tag then append the token as it is.\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "             #Else use the POS tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag).lower())\n",
    "    return \" \".join(lemmatized_sentence)          #returning the lematized sentance\n",
    "\n",
    "\n",
    "X['Regtoken_cleaned_txt']=X[0].apply(Regextoken_lemmatize_sentence) #calling the above function(Regextoken_lemmatize_sentence) and stores the values to X dataset in a new column named as Regtoken_cleaned_txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Wordtoken_cleaned_txt</th>\n",
       "      <th>TreeToken_cleaned_txt</th>\n",
       "      <th>Regtoken_cleaned_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>so way plug us unless I go converter .</td>\n",
       "      <td>so way plug us unless I go converter .</td>\n",
       "      <td>way plug us unless go converter .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>good case , excellent value .</td>\n",
       "      <td>good case , excellent value .</td>\n",
       "      <td>good case , excellent value .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>great jawbone .</td>\n",
       "      <td>great jawbone .</td>\n",
       "      <td>great jawbone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>tied charger conversation last 45 minutes.majo...</td>\n",
       "      <td>tied charger conversation last 45 minutes.majo...</td>\n",
       "      <td>tie charger conversation last 45 minute .major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>The mic great .</td>\n",
       "      <td>The mic great .</td>\n",
       "      <td>mic great .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The screen get smudge easily touch ear face .</td>\n",
       "      <td>The screen get smudge easily touch ear face .</td>\n",
       "      <td>screen get smudge easily touch ear face .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>What piece junk .. I lose call phone .</td>\n",
       "      <td>What piece junk .. I lose call phone .</td>\n",
       "      <td>piece junk .. lose call phone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "      <td>item does not match picture .</td>\n",
       "      <td>item does not match picture .</td>\n",
       "      <td>item match picture .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "      <td>The thing disappoint infra red port ( irda ) .</td>\n",
       "      <td>The thing disappoint infra red port ( irda ) .</td>\n",
       "      <td>thing disappoint infra red port ( irda ) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>You answer call unit , never work !</td>\n",
       "      <td>You answer call unit , never work !</td>\n",
       "      <td>answer call unit , never work !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0    So there is no way for me to plug it in here i...  0   \n",
       "1                          Good case, Excellent value.  1   \n",
       "2                               Great for the jawbone.  1   \n",
       "3    Tied to charger for conversations lasting more...  0   \n",
       "4                                    The mic is great.  1   \n",
       "..                                                 ... ..   \n",
       "995  The screen does get smudged easily because it ...  0   \n",
       "996  What a piece of junk.. I lose more calls on th...  0   \n",
       "997                       Item Does Not Match Picture.  0   \n",
       "998  The only thing that disappoint me is the infra...  0   \n",
       "999  You can not answer calls with the unit, never ...  0   \n",
       "\n",
       "                                 Wordtoken_cleaned_txt  \\\n",
       "0               so way plug us unless I go converter .   \n",
       "1                        good case , excellent value .   \n",
       "2                                      great jawbone .   \n",
       "3    tied charger conversation last 45 minutes.majo...   \n",
       "4                                      The mic great .   \n",
       "..                                                 ...   \n",
       "995      The screen get smudge easily touch ear face .   \n",
       "996             What piece junk .. I lose call phone .   \n",
       "997                      item does not match picture .   \n",
       "998     The thing disappoint infra red port ( irda ) .   \n",
       "999                You answer call unit , never work !   \n",
       "\n",
       "                                 TreeToken_cleaned_txt  \\\n",
       "0               so way plug us unless I go converter .   \n",
       "1                        good case , excellent value .   \n",
       "2                                      great jawbone .   \n",
       "3    tied charger conversation last 45 minutes.majo...   \n",
       "4                                      The mic great .   \n",
       "..                                                 ...   \n",
       "995      The screen get smudge easily touch ear face .   \n",
       "996             What piece junk .. I lose call phone .   \n",
       "997                      item does not match picture .   \n",
       "998     The thing disappoint infra red port ( irda ) .   \n",
       "999                You answer call unit , never work !   \n",
       "\n",
       "                                  Regtoken_cleaned_txt  \n",
       "0                    way plug us unless go converter .  \n",
       "1                        good case , excellent value .  \n",
       "2                                      great jawbone .  \n",
       "3    tie charger conversation last 45 minute .major...  \n",
       "4                                          mic great .  \n",
       "..                                                 ...  \n",
       "995          screen get smudge easily touch ear face .  \n",
       "996                    piece junk .. lose call phone .  \n",
       "997                               item match picture .  \n",
       "998         thing disappoint infra red port ( irda ) .  \n",
       "999                    answer call unit , never work !  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#print(\"The Dataset after applying the tokenizers, POS tagging, removing stop words and Lematizing\", X)\n",
    "print(\"------------------\"*10)\n",
    "\n",
    "'''\n",
    "#The function cleantxt() as represented below makes sure only character from a-z \n",
    "#and A-Z are present and remaining ones are removed from the pre-processed dataset. \n",
    "#We apply this to all the 3 cleaned columns in the dataset X for each of the tokenizer used.'''\n",
    "\n",
    "def cleantext(retext):\n",
    "    return re.sub('[^a-zA-Z]',' ',str(retext))#.lower()\n",
    "X['TreeToken_cleaned_txt']=X['TreeToken_cleaned_txt'].apply(cleantext)\n",
    "X['Regtoken_cleaned_txt']=X['Regtoken_cleaned_txt'].apply(cleantext)\n",
    "X['Wordtoken_cleaned_txt']=X['Wordtoken_cleaned_txt'].apply(cleantext)\n",
    "#X_test['cleanedtxt']=X_test['Regtokenizedtxt'].apply(cleantext)\n",
    "#The function removechar() as represented below removes the new line character if any present,\n",
    "# if any apostrophes,hyphens,quotation marks,etc.\n",
    "def removechar(text):\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    return text\n",
    "                 \n",
    "#The below removechar is called for all the 3 columns in X dataset.\n",
    "X['TreeToken_cleaned_txt']=X['TreeToken_cleaned_txt'].apply(removechar)\n",
    "X['Regtoken_cleaned_txt']=X['Regtoken_cleaned_txt'].apply(removechar)\n",
    "X['Wordtoken_cleaned_txt']=X['Wordtoken_cleaned_txt'].apply(removechar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The dataset is split as: For Training 66.6% and for Testing as 33.3%. \n",
      "The sizes of: X_Train=670\n",
      "X_Test=330\n",
      "y_train=670\n",
      "y_test=330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "             (B) Splitting Dataset/Corpus:\n",
    "    #The dataset is split into 33.3% for test and 66.6%. This is taken randomly \n",
    "    any other split for test and train data is possible.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['TreeToken_cleaned_txt'], target, test_size=0.33)   #test and train data split for the data to which treebank token is applied\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X['Regtoken_cleaned_txt'], target, test_size=0.33)      #test and train data split for the data to which Regextoken is applied\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X['Wordtoken_cleaned_txt'], target, test_size=0.33)     #test and train data split for the data to which wordtoken is applied\n",
    "print(\" The dataset is split as: For Training 66.6% and for Testing as 33.3%. \\nThe sizes of: X_Train={0}\\nX_Test={1}\\ny_train={2}\\ny_test={3}\".format(X_train.size,X_test.size,y_train.size,y_test.size))\n",
    "\n",
    "# As the X_train,y_train,X_train2,...,etc are of type pandas.series.series, but \n",
    "#for passing these values to CountVectorizer it needs to be of type pandas.Dataframe. \n",
    "#Hence, converting the type.\n",
    "X_train=X_train.to_frame()\n",
    "y_train=y_train.to_frame()\n",
    "X_test=X_test.to_frame()\n",
    "y_test=y_test.to_frame()\n",
    "\n",
    "X_train2=X_train2.to_frame()\n",
    "y_train2=y_train2.to_frame()\n",
    "X_test2=X_test2.to_frame()\n",
    "y_test2=y_test2.to_frame()\n",
    "\n",
    "X_train3=X_train3.to_frame()\n",
    "y_train3=y_train3.to_frame()\n",
    "X_test3=X_test3.to_frame()\n",
    "y_test3=y_test3.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                (C) Feature Engineering\n",
    "        Bag of Words:The important part is to find the features from the data to make\n",
    "        machine learning algorithms works. In this case, we have text. We need to convert this \n",
    "        text into numbers that we can do calculations on. We use word frequencies. That is treating\n",
    "        every document as a set of the words it contains. Our features will be the counts of each of \n",
    "        these words.\n",
    "            The term-document matrix is constructed in the block below for each of the 3 tokenizers.\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer()\n",
    "count_vect2=CountVectorizer()\n",
    "count_vect3=CountVectorizer()\n",
    "\n",
    "counts=count_vect.fit_transform(X_train['TreeToken_cleaned_txt'])   #fitting the data\n",
    "counts2=count_vect2.fit_transform(X_train2['Regtoken_cleaned_txt'])\n",
    "counts3=count_vect3.fit_transform(X_train3['Wordtoken_cleaned_txt'])\n",
    "\n",
    "counts_test=count_vect.transform(X_test[\"TreeToken_cleaned_txt\"])   #transform the data\n",
    "counts_test2=count_vect2.transform(X_test2[\"Regtoken_cleaned_txt\"])\n",
    "counts_test3=count_vect3.transform(X_test3[\"Wordtoken_cleaned_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "For Multinomial Naive Bayers Classifier for Continuous Bag of Words (CBOW)\n",
      "Accuracy for NB using TreeBankTokenizer : 0.803030303030303\n",
      "Accuracy for NB using RegExp Tokenizer: 0.796969696969697\n",
      "Accuracy for NB using Word Tokenizer: 0.7787878787878788\n",
      "--------------------------------------------------------------------------------\n",
      "Classification report for Naive Bayers Classifier using WordTokenizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.79       170\n",
      "           1       0.76      0.88      0.81       160\n",
      "\n",
      "    accuracy                           0.80       330\n",
      "   macro avg       0.81      0.81      0.80       330\n",
      "weighted avg       0.81      0.80      0.80       330\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification report for Naive Bayers Classifier using RegexTokenizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77       161\n",
      "           1       0.76      0.89      0.82       169\n",
      "\n",
      "    accuracy                           0.80       330\n",
      "   macro avg       0.81      0.79      0.79       330\n",
      "weighted avg       0.81      0.80      0.79       330\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification report for Naive Bayers Classifier using Treetokenizer\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.76       167\n",
      "           1       0.74      0.85      0.79       163\n",
      "\n",
      "    accuracy                           0.78       330\n",
      "   macro avg       0.79      0.78      0.78       330\n",
      "weighted avg       0.79      0.78      0.78       330\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "For Random Forest Classifier for Continuous Bag of Words (CBOW) considering TreeBankTokenizer\n",
      "The accuracy for RandomForest Classifier is : 0.806060606060606\n",
      "The F1-Score for RandomForest Classifier is : 0.803979803979804\n"
     ]
    }
   ],
   "source": [
    "'''               (D) and (E)  Classifiers construction and evaluation :Different estimators are better \n",
    "            suited for different types of data and different problems. The Naive Byers is chosen for this\n",
    "            dataset as it relies on a very simple representation of the document (called the bag of words representation). \n",
    "            Also, it recommended on sklearn cheatsheet that if the dataset is <100k and it's text then Naive \n",
    "            Bayers is a good option. However other algorithms maybe applied as well.\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()\n",
    "\n",
    "nb.fit(counts, y_train)         #fitting the model for Treebank tokenizer model\n",
    "\n",
    "nb2=MultinomialNB()\n",
    "nb2.fit(counts2, y_train2)          #fitting the model for Regex tokenizer model\n",
    "\n",
    "nb3=MultinomialNB()\n",
    "nb3.fit(counts3, y_train3)      #fitting the model for word tokenizer model\n",
    "print(\"-----------\"*10)\n",
    "print(\"For Multinomial Naive Bayers Classifier for Continuous Bag of Words (CBOW)\")\n",
    "print(\"Accuracy for NB using TreeBankTokenizer :\", nb.score(counts_test, y_test))\n",
    "print(\"Accuracy for NB using RegExp Tokenizer:\", nb2.score(counts_test2, y_test2))\n",
    "print(\"Accuracy for NB using Word Tokenizer:\", nb3.score(counts_test3, y_test3))\n",
    "y_pred=nb.predict(counts_test)      #evaluating the test set\n",
    "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score,classification_report  #importing all the metrics\n",
    "confusion_matrix(y_test, y_pred)            # confusion matrix also known as error matrix, allows visualization of the performance of an algorithm\n",
    "\n",
    "y_pred2=nb2.predict(counts_test2)           #evaluating the test set for Regex tokenizer model\n",
    "y_pred3=nb3.predict(counts_test3)           #evaluating the test set for word tokenizer model\n",
    "f1_score(y_test,y_pred, average=\"macro\" )\n",
    "print(\"--------\"*10)\n",
    "print(\"Classification report for Naive Bayers Classifier using WordTokenizer\\n\",classification_report(y_test,y_pred))\n",
    "print(\"--------\"*10)\n",
    "print(\"Classification report for Naive Bayers Classifier using RegexTokenizer\\n\",classification_report(y_test2,y_pred2))\n",
    "print(\"--------\"*10)\n",
    "print(\"Classification report for Naive Bayers Classifier using Treetokenizer\\n\",classification_report(y_test3,y_pred3))\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, bootstrap=True, max_features='sqrt')\n",
    "clf.fit(counts, y_train)                #fitting the Random Forest Model\n",
    "y_pred=clf.predict(counts_test)         #evaluating the test set for random forest. Note: After here on only TreeBank tokenizer model\n",
    "print(\"-----------\"*10)\n",
    "print(\"For Random Forest Classifier for Continuous Bag of Words (CBOW) considering TreeBankTokenizer\")\n",
    "print(\"The accuracy for RandomForest Classifier is :\", accuracy_score(y_test, y_pred))\n",
    "print(\"The F1-Score for RandomForest Classifier is :\", f1_score(y_test,y_pred, average=\"macro\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "For Multinomial Naive Bayers Classifier for TF-iDF, considering TreeBankTokenizer\n",
      "The accuracy for Multinomial Naive Bayers Classifier is : 0.793939393939394\n",
      "The F1-Score for Multinomial Naive Bayers Classifier is : 0.7924451515039402\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "For Random Forest Classifier for TF-iDF, considering TreeBankTokenizer\n",
      "The accuracy for RandomForest Classifier is : 0.806060606060606\n",
      "The F1-Score for RandomForest Classifier is : 0.803979803979804\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "                    Feature Engineering\n",
    "            TF-IDF: A term-Document Matrix was constructed from the CountVectorizer() containing words or terms along columns and sentences along rows.\n",
    "    The bag of words only considers the count of words which is not a good practice for doing sentiment analysis. \n",
    "    Because some common words appear in many sentences which contain less importantnce. \n",
    "    Therefore,TF-IDF is used in this assignment for Naive Bayers and Random Forest Algorithms\n",
    "    which takes into account the word based upon its uniqueness.\n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(counts)     #counts is the count of words from Bag of words. To which we fit and transform.\n",
    "X_train_tfidf.shape                                     #Displayes the dimensions of the variable\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1 = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "y_pred1 = clf1.predict(counts_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred1)                 #evaluating the test set\n",
    "print(\"-----------\"*10)\n",
    "print(\"For Multinomial Naive Bayers Classifier for TF-iDF, considering TreeBankTokenizer\")\n",
    "print(\"The accuracy for Multinomial Naive Bayers Classifier is :\", accuracy_score(y_test, y_pred1))\n",
    "print(\"The F1-Score for Multinomial Naive Bayers Classifier is :\", f1_score(y_test,y_pred1, average=\"macro\" ))\n",
    "clf_1_randomforest= RandomForestClassifier(n_estimators=1000, bootstrap=True, max_features='sqrt')\n",
    "clf_1_randomforest.fit(X_train_tfidf, y_train)   #fitting the Random Forest Model\n",
    "y_pred1=clf.predict(counts_test)                    #evaluating the test set for random forest. \n",
    "accuracy_score(y_test, y_pred1)                     #accuracy sore for the model \n",
    "print(\"-----------\"*10)\n",
    "print(\"For Random Forest Classifier for TF-iDF, considering TreeBankTokenizer\")\n",
    "print(\"The accuracy for RandomForest Classifier is :\", accuracy_score(y_test, y_pred1))\n",
    "print(\"The F1-Score for RandomForest Classifier is :\", f1_score(y_test,y_pred1, average=\"macro\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
